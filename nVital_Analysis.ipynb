{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04d19105",
   "metadata": {},
   "source": [
    "# nVital Analysis Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8842d2",
   "metadata": {},
   "source": [
    "# Full Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb09f90",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2f5d42",
   "metadata": {},
   "source": [
    "### Environment Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba3c3fe",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up environment\n",
    "%matplotlib\n",
    "%matplotlib\n",
    "\n",
    "# Imports\n",
    "import platform\n",
    "import matplotlib#must be before all other matplotlib imports\n",
    "#matplotlib.use('Qt5Agg')\n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.dates as md\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from typing import NamedTuple\n",
    "from scipy import signal\n",
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "import sys\n",
    "import math\n",
    "from pathlib import Path\n",
    "import os\n",
    "from glob import glob\n",
    "from scipy.signal import savgol_filter, resample\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from tkinter import filedialog, Tk\n",
    "from matplotlib.widgets import Slider\n",
    "from scipy.interpolate import CubicSpline, make_interp_spline\n",
    "import re\n",
    "import psutil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eac868",
   "metadata": {},
   "source": [
    "### Settings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cc56a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Master Toggles\n",
    "export_to_csv = True                 #toggle off all export data to csvs\n",
    "plot_output = True                   #toggle off all plotting\n",
    "\n",
    "# ANSI escape code for text styling\n",
    "BOLD = \"\\033[1m\"\n",
    "GREEN = \"\\033[32m\"  # Green text\n",
    "RED = \"\\033[31m\"    # Red text\n",
    "DEFAULT = \"\\033[0m\"\n",
    "\n",
    "sampling_rate = 800\n",
    "window = 1 #For activity calculations\n",
    "\n",
    "#Temperature plot\n",
    "plot_temperature = True            #plot temperature\n",
    "\n",
    "#Acceleration plots\n",
    "plot_accel_x = False                #plot x acceleration\n",
    "plot_accel_y = False                #plot y acceleration\n",
    "plot_accel_z = False                #plot z acceleration\n",
    "\n",
    "#Activity plot\n",
    "plot_activity = True                #plot activity\n",
    "\n",
    "#Cardiac plots\n",
    "plot_accel_z_clean = False          #plot z acceleration with motion artifacts removed\n",
    "plot_hs = False                     #plot heart sound \n",
    "plot_hs_sh_raw = False              #plot shannon energy envelope of heart sound\n",
    "plot_hs_sh_filtered = False         #plot shannon energy envelope of heart sound with LPF applied\n",
    "plot_hs_lt = False                  #plot length transform of heart sound\n",
    "plot_HR_peaks = False               #plot HR peaks\n",
    "plot_HR = True                      #plot heart rate\n",
    "plot_SQI = True                     #plot signal quality index\n",
    "\n",
    "#Respiratory plots\n",
    "use_hs_to_calculate_RR = False         #use heart sound to calculate respiratory rate\n",
    "use_shannon_peaks_only_to_calculate_resp_env = True #If false, use both shannon peaks and length transform peaks to calculate respiratory envelope\n",
    "plot_resp_sig_prefilt = False          #plot the respiratory signal before bpf applied\n",
    "overlay_resp_sig_prefilt_w_hs = False  #Set to true to plot respiratory signal before bpf applied on top of hs_sh_filtered\n",
    "plot_resp_sig = False                  #plot the respiratory signal after bpf applied\n",
    "plot_RR_peaks = False                  #plot peaks of the respiratory signal\n",
    "plot_RR = True                        #plot respiratory rate\n",
    "\n",
    "#Debugging plots:\n",
    "plot_SR = False                       #Sampling Rate of incoming data #Does not work yet\n",
    "plot_std_xx = False                   #Standard Deviation of acceleration #Does not work yet\n",
    "\n",
    "#Plot settings:\n",
    "scrolling = False                     #produce additional interactive figure with scrolling \n",
    "scroll_window = 2                     #scrolling window size (seconds)\n",
    "\n",
    "####################################################################################################################\n",
    "\n",
    "class paramstruct(NamedTuple):\n",
    "    fs: int\n",
    "    total_time: float\n",
    "    # HR Parameters\n",
    "    vital_w: int\n",
    "    hs: tuple\n",
    "    hs_len: float\n",
    "    hr: tuple\n",
    "    hs_thresh: tuple\n",
    "    activity_thresh: tuple\n",
    "    min_scale: int\n",
    "    max_scale: int\n",
    "    min_scale_resp: int\n",
    "    max_scale_resp: int\n",
    "    fs_ds: int\n",
    "    resp: tuple \n",
    "    resp_axis: str\n",
    "    ds_resp: int\n",
    "    fs_resp: int\n",
    "\n",
    "params = paramstruct\n",
    "params.fs = sampling_rate        \n",
    "        \n",
    "#Set up HR and RR Calculation Parameters\n",
    "params.hs = (25, 390) #Original\n",
    "params.vital_w = 4 #8 #60 # vital sign calculation window in seconds\n",
    "params.vital_ovlp = 0.25 # overlap of windows #1 is no overlap, 0 is complete overlap\n",
    "params.hs_len = .165 # max length (sec) of heart sound (Luisada, Mendoza, Alimurung (1948))\n",
    "params.hs_thresh = (.00001,.3) \n",
    "params.hr = (400, 900) # min and max heart rate expected\n",
    "params.activity_thresh = (.00001,.5)\n",
    "params.downsample = 400 # frequency to downsample sensor data for vitals analysis\n",
    "params.resp = (1.25, 3.0) # respiratory frequency #original\n",
    "params.resp_axis = 'x' # axis to use to analyze respiration data (recommend x)\n",
    "params.ds_resp = 50. # frequency to downsample sensor data for respiration analysis\n",
    "\n",
    "params.resp_peak_height = None\n",
    "params.resp_peak_prominence = 0.00002\n",
    "params.resp_peak_width = None\n",
    "params.resp_peak_wlen = None\n",
    "\n",
    "disconnection_length = 1000 # create a new chunk if the device disconnects for more than this length (ms)\n",
    "min_chunk_length = 60 # discard chunks shorter than this length (s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f822f8",
   "metadata": {},
   "source": [
    "## Select and Preprocess Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45abf57c",
   "metadata": {},
   "source": [
    "### Select and Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8dfc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data (Select the folder containing the Accel/Temp/Event CSVs)\n",
    "\n",
    "def select_folder_and_files():\n",
    "    root = Tk()\n",
    "    root.withdraw()\n",
    "    root.attributes(\"-topmost\", True)\n",
    "    root.update()\n",
    "\n",
    "    folder_path = filedialog.askdirectory(title=\"Select a Folder\")\n",
    "\n",
    "    if not folder_path:\n",
    "        return []\n",
    "\n",
    "    files_in_folder = os.listdir(folder_path)\n",
    "\n",
    "    csv_files = [os.path.join(folder_path, f).replace(\"\\\\\", \"/\") for f in files_in_folder if f.endswith('.csv')]\n",
    "    pq_files = [os.path.join(folder_path, f).replace(\"\\\\\", \"/\") for f in files_in_folder if f.endswith('.pq')]\n",
    "\n",
    "    if csv_files and pq_files:\n",
    "        print(\"The folder contains a mix of CSV and PQ files. Please select a valid folder.\")\n",
    "        return []\n",
    "\n",
    "    return folder_path, csv_files if csv_files else pq_files\n",
    "\n",
    "def process_csv_files(file_paths):\n",
    "    print(f\"before process_csv_files RAM used: {psutil.Process().memory_info().rss / 1e9:.2f} GB\")\n",
    "    accel_files = [f for f in file_paths if '_Accel_' in f and f.endswith('.csv')]\n",
    "    temp_files = [f for f in file_paths if '_Temp_' in f and f.endswith('.csv')]\n",
    "    event_files = [f for f in file_paths if '_Event_' in f and f.endswith('.csv')]\n",
    "\n",
    "    def get_file_number(filename):\n",
    "        return int(filename.split('_')[-1].replace('.csv', ''))\n",
    "\n",
    "    accel_files.sort(key=get_file_number)\n",
    "    temp_files.sort(key=get_file_number)\n",
    "    event_files.sort(key=get_file_number)\n",
    "\n",
    "\n",
    "    accel_data_list = []\n",
    "    temp_data_list = []\n",
    "    event_data_list = []\n",
    "\n",
    "    for file in accel_files:\n",
    "        df = pd.read_csv(file)\n",
    "        \n",
    "        df_rows = df.shape[0]\n",
    "        df_values = []\n",
    "        for i, row in enumerate(df.values):\n",
    "            df_values.append(row)\n",
    "            if i % 1000 == 0:\n",
    "                print(f\"Extracted {i + 1} out of {df_rows} rows from accel CSV\")\n",
    "        \n",
    "        accel_data_list.append(df_values)\n",
    "    \n",
    "    for file in temp_files:\n",
    "        df = pd.read_csv(file)\n",
    "        \n",
    "        df_rows = df.shape[0]\n",
    "        df_values = []\n",
    "        for i, row in enumerate(df.values):\n",
    "            df_values.append(row)\n",
    "            if i % 1000 == 0:\n",
    "                print(f\"Extracted {i + 1} out of {df_rows} rows from temp CSV\")\n",
    "        \n",
    "        temp_data_list.append(df_values)\n",
    "\n",
    "    for file in event_files:\n",
    "        df = pd.read_csv(file)\n",
    "        \n",
    "        df_rows = df.shape[0]\n",
    "        df_values = []\n",
    "        for i, row in enumerate(df.values):\n",
    "            df_values.append(row)\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Extracted {i + 1} out of {df_rows} rows from event CSV\")\n",
    "        \n",
    "        event_data_list.append(df_values)\n",
    "\n",
    "    accel_data = np.concatenate(accel_data_list) if accel_data_list else np.array([])\n",
    "    temp_data = np.concatenate(temp_data_list) if temp_data_list else np.array([])\n",
    "    event_data = np.concatenate(event_data_list) if event_data_list else np.array([])\n",
    "\n",
    "    datetime_format = \"%Y-%m-%d %H:%M:%S.%f %z\"\n",
    "\n",
    "    if len(accel_data) > 0:\n",
    "        accel_data_rows = accel_data.shape[0]\n",
    "        accel_data_time = []\n",
    "        \n",
    "        for i, t in enumerate(accel_data[:, 0]):\n",
    "            accel_data_time.append(datetime.strptime(t, datetime_format))\n",
    "            if i % 1000 == 0:\n",
    "                print(f\"Finished converting {i + 1} out of {accel_data_rows} accel_data rows to datetimes\")\n",
    "        \n",
    "        accel_data = np.column_stack((\n",
    "            accel_data_time,\n",
    "            accel_data[:, 1:]\n",
    "        ))\n",
    "\n",
    "    print(\"Processed accelerometer files\")\n",
    "\n",
    "    if len(temp_data) > 0:\n",
    "        temp_data_rows = temp_data.shape[0]\n",
    "        temp_data_time = []\n",
    "        \n",
    "        for i, t in enumerate(temp_data[:, 0]):\n",
    "            temp_data_time.append(datetime.strptime(t, datetime_format))\n",
    "            if i % 1000 == 0:\n",
    "                print(f\"Finished converting {i + 1} out of {temp_data_rows} temp_data rows to datetimes\")\n",
    "        \n",
    "        temp_data = np.column_stack((\n",
    "            temp_data_time,\n",
    "            temp_data[:, 1:]\n",
    "        ))\n",
    "        \n",
    "    print(\"Processed temperature files\")\n",
    "        \n",
    "    if len(event_data) > 0:\n",
    "        event_data_rows = event_data.shape[0]\n",
    "        event_data_time = []\n",
    "        \n",
    "        for i, t in enumerate(event_data[:, 0]):\n",
    "            event_data_time.append(datetime.strptime(t, datetime_format))\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Finished converting {i + 1} out of {event_data_rows} event_data rows to datetimes\")\n",
    "        \n",
    "        event_data = np.column_stack((\n",
    "            event_data_time,\n",
    "            event_data[:, 1:]\n",
    "        ))    \n",
    "        \n",
    "    print(\"Processed event files\")    \n",
    "        \n",
    "    print(f\"process_csv_files RAM used: {psutil.Process().memory_info().rss / 1e9:.2f} GB\")\n",
    "    \n",
    "    return accel_data, temp_data, event_data\n",
    "\n",
    "def process_pq_files(file_paths):\n",
    "    accel_file = next((f for f in file_paths if 'stream-0x30.pq' in f), None)\n",
    "    temp_file = next((f for f in file_paths if 'stream-0x13.pq' in f), None)\n",
    "\n",
    "    accel_data = pd.read_parquet(accel_file).values if accel_file else np.array([])\n",
    "    temp_data = pd.read_parquet(temp_file).values if temp_file else np.array([])\n",
    "    event_data = np.array([])\n",
    "    \n",
    "    print(\"Processed accelerometer files\")\n",
    "    print(\"Processed temperature files\")\n",
    "    \n",
    "    return accel_data, temp_data, event_data\n",
    "\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / 1e9:.2f} GB\")\n",
    "\n",
    "os_name = platform.system()\n",
    "if os_name == \"Windows\":\n",
    "    folder_path, file_paths = select_folder_and_files()\n",
    "else: #Mac\n",
    "    folder_path = 'your/folder/path/here' \n",
    "    \n",
    "    files_in_folder = os.listdir(folder_path)\n",
    "    file_paths = [os.path.join(folder_path, f).replace(\"\\\\\", \"/\") for f in files_in_folder if f.endswith('.csv')]\n",
    "\n",
    "folder_name = os.path.basename(folder_path)\n",
    "csv_not_pq = True\n",
    "\n",
    "print(\"loading files\")\n",
    "\n",
    "if file_paths:\n",
    "    if all(f.endswith('.csv') for f in file_paths):\n",
    "        accel_data, temp_data, event_data = process_csv_files(file_paths)\n",
    "    elif all(f.endswith('.pq') for f in file_paths):\n",
    "        csv_not_pq = False\n",
    "        accel_data, temp_data, event_data = process_pq_files(file_paths)\n",
    "    else:\n",
    "        print(\"No valid files were selected.\")\n",
    "else:\n",
    "    print(\"No valid files were selected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ea0a02",
   "metadata": {},
   "source": [
    "### Chunk Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd181bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate data into chunks\n",
    "\n",
    "def remove_duplicates_from_data(accel_data):\n",
    "    mask = np.r_[True, accel_data[1:,0] > accel_data[:-1,0]]\n",
    "    accel_data_clean = accel_data[mask]\n",
    "    num_removed = len(accel_data) - len(accel_data_clean)\n",
    "    print(f\"Removed {num_removed} duplicate entries\")\n",
    "    \n",
    "    return accel_data_clean\n",
    "\n",
    "accel_data = remove_duplicates_from_data(accel_data)\n",
    "\n",
    "accel_first_dt = accel_data[0, 0]\n",
    "if csv_not_pq:\n",
    "    accel_first_ms = accel_first_dt.timestamp() * 1000\n",
    "else:\n",
    "    accel_first_ms = accel_first_dt * 1000\n",
    "accel_ms = np.round([(dt.timestamp() * 1000 - accel_first_ms) for dt in accel_data[:, 0]], decimals=3)\n",
    "accel_ms_diff = np.diff(accel_ms)\n",
    "chunk_end = np.argwhere(accel_ms_diff > disconnection_length)\n",
    "chunk_start = [0]\n",
    "chunk_start = np.append(chunk_start, chunk_end + 1)\n",
    "chunk_end = np.append(chunk_end, len(accel_ms_diff))\n",
    "chunk_mask = chunk_end - chunk_start > params.fs * min_chunk_length\n",
    "chunk_start = chunk_start[chunk_mask]\n",
    "chunk_end = chunk_end[chunk_mask]\n",
    "num_chunks = len(chunk_start)\n",
    "\n",
    "accel_x = accel_data[:,1]\n",
    "accel_y = accel_data[:,2]\n",
    "accel_z = accel_data[:,3]\n",
    "\n",
    "temp_s = np.round([(dt.timestamp() - accel_first_ms/1000) for dt in temp_data[:, 0]], decimals=3)\n",
    "temp = temp_data[:, 1] if csv_not_pq else temp_data[:, 1] / 1000\n",
    "\n",
    "if event_data.size == 0:\n",
    "    event_ms = []\n",
    "    event = []\n",
    "else:\n",
    "    event_ms = np.round([(dt.timestamp() * 1000 - accel_first_ms) for dt in event_data[:, 0]], decimals=3)\n",
    "    event = event_data[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b037ac8",
   "metadata": {},
   "source": [
    "### Resample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eda99dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample Acceleration Data to 800 Hz\n",
    "\n",
    "accel_ms_old = accel_ms\n",
    "new_chunk_start = [0]\n",
    "new_chunk_end = []\n",
    "new_accel_ms = []\n",
    "new_accel_x = []\n",
    "new_accel_y = []\n",
    "new_accel_z = []\n",
    "\n",
    "for chunk in range(num_chunks):\n",
    "    print(f\"Resampling chunk {chunk+1} out of {num_chunks}\")\n",
    "    new_start_time = accel_ms[chunk_start[chunk]]\n",
    "    new_num_samples = int(np.floor(params.fs*(accel_ms[chunk_end[chunk]] - new_start_time)/1000) + 1)\n",
    "    new_end_time = accel_ms[chunk_start[chunk]] + round((1000*(new_num_samples - 1)/params.fs), 3)\n",
    "\n",
    "    new_accel_ms_chunk = np.linspace(new_start_time, new_end_time, new_num_samples)\n",
    "    new_accel_ms = np.append(new_accel_ms, new_accel_ms_chunk)\n",
    "\n",
    "    if chunk > 0:\n",
    "        new_chunk_start = np.append(new_chunk_start, (new_chunk_end[chunk-1] + 1))\n",
    "        new_chunk_end = np.append(new_chunk_end, (new_chunk_end[chunk - 1] + (new_num_samples))).astype(int)\n",
    "    else:\n",
    "        new_chunk_end = np.append(new_chunk_end, (new_chunk_start[chunk] + (new_num_samples - 1))).astype(int)\n",
    "\n",
    "    cs_x = CubicSpline(accel_ms[chunk_start[chunk]:chunk_end[chunk]], accel_x[chunk_start[chunk]:chunk_end[chunk]])\n",
    "    cs_y = CubicSpline(accel_ms[chunk_start[chunk]:chunk_end[chunk]], accel_y[chunk_start[chunk]:chunk_end[chunk]])\n",
    "    cs_z = CubicSpline(accel_ms[chunk_start[chunk]:chunk_end[chunk]], accel_z[chunk_start[chunk]:chunk_end[chunk]])\n",
    "\n",
    "    new_accel_x_chunk = cs_x(new_accel_ms_chunk)\n",
    "    new_accel_y_chunk = cs_y(new_accel_ms_chunk)\n",
    "    new_accel_z_chunk = cs_z(new_accel_ms_chunk)\n",
    "\n",
    "    new_accel_x = np.append(new_accel_x, new_accel_x_chunk)\n",
    "    new_accel_y = np.append(new_accel_y, new_accel_y_chunk)\n",
    "    new_accel_z = np.append(new_accel_z, new_accel_z_chunk)\n",
    "\n",
    "accel_dt = [accel_first_dt + pd.Timedelta(milliseconds=ms) for ms in new_accel_ms]\n",
    "    \n",
    "chunk_start = new_chunk_start  \n",
    "chunk_end = new_chunk_end\n",
    "accel_ms = new_accel_ms\n",
    "accel_x = new_accel_x\n",
    "accel_y = new_accel_y\n",
    "accel_z = new_accel_z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49611b4",
   "metadata": {},
   "source": [
    "## Bluetooth Conctivity Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aad16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export BLE connectivity metrics\n",
    "\n",
    "def create_connection_status_csv(accel_dt, chunk_start, chunk_end, output_path):\n",
    "    ble_status = []\n",
    "    start_times = []\n",
    "    stop_times = []\n",
    "    total_times = []\n",
    "    total_times_s = []\n",
    "\n",
    "    overall_start_time = accel_dt[chunk_start[0]]\n",
    "    overall_end_time = accel_dt[chunk_end[-1]]\n",
    "    hours_rounded = int(((overall_end_time - overall_start_time).total_seconds()) / 3600)\n",
    "\n",
    "    hourly_conn_rates = []\n",
    "\n",
    "    for hour in range(hours_rounded):\n",
    "        hour_start_time = overall_start_time + timedelta(hours=hour)\n",
    "        hour_stop_time = hour_start_time + timedelta(hours=1)\n",
    "\n",
    "        connection_time = timedelta(seconds=0)\n",
    "        total_time = timedelta(seconds=0)\n",
    "\n",
    "        for i in range(len(chunk_start)):\n",
    "            chunk_start_time = accel_dt[chunk_start[i]]\n",
    "            chunk_stop_time = accel_dt[chunk_end[i]]\n",
    "            \n",
    "            if chunk_start_time < hour_stop_time:\n",
    "                if chunk_stop_time > hour_start_time:\n",
    "                    chunk_in_hour_start = max(chunk_start_time, hour_start_time)\n",
    "                    chunk_in_hour_end = min(chunk_stop_time, hour_stop_time)\n",
    "                    connection_time += chunk_in_hour_end - chunk_in_hour_start\n",
    "                    #total_time += timedelta(hours=1)\n",
    "        \n",
    "        connection_rate = 100 * connection_time.total_seconds() / 3600\n",
    "        \n",
    "        hourly_conn_rates.append({\n",
    "            'hour': hour + 1,\n",
    "            'start_time': hour_start_time,\n",
    "            'end_time': hour_stop_time,\n",
    "            'connection_rate': round(connection_rate, 3)\n",
    "        })\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        start_time = accel_dt[chunk_start[i]]\n",
    "        stop_time = accel_dt[chunk_end[i]]\n",
    "        \n",
    "        ble_status.append(\"Connected\")\n",
    "        start_times.append(start_time)\n",
    "        stop_times.append(stop_time)\n",
    "        total_times.append(stop_time - start_time)\n",
    "        total_times_s.append((stop_time - start_time).total_seconds())\n",
    "    \n",
    "    for i in range(num_chunks - 1):\n",
    "        start_time = accel_dt[chunk_end[i]]\n",
    "        stop_time = accel_dt[chunk_start[i + 1]]\n",
    "        \n",
    "        ble_status.append(\"Disconnected\")\n",
    "        start_times.append(start_time)\n",
    "        stop_times.append(stop_time)\n",
    "        total_times.append(stop_time - start_time)\n",
    "        total_times_s.append((stop_time - start_time).total_seconds())\n",
    "    \n",
    "    total_duration = sum(total_times_s)\n",
    "    percent_times = [round((t / total_duration) * 100, 3) for t in total_times_s]\n",
    "\n",
    "    hourly_df = pd.DataFrame(hourly_conn_rates)\n",
    "\n",
    "    chunk_df = pd.DataFrame({\n",
    "        'ble_status': ble_status,\n",
    "        'start_time': start_times,\n",
    "        'stop_time': stop_times,\n",
    "        'total_time': total_times,\n",
    "        'percent_time': percent_times\n",
    "    })\n",
    "    \n",
    "    chunk_df = chunk_df.sort_values('start_time')\n",
    "\n",
    "    summary_df = chunk_df.groupby('ble_status').agg({\n",
    "        'total_time': 'sum',\n",
    "        'percent_time': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    with open(output_path, 'w', newline='') as f:\n",
    "        f.write(\"Connection Summary\\n\")\n",
    "        summary_df.to_csv(f, index=False)\n",
    "        f.write(\"\\nHourly Connection Rates\\n\")\n",
    "        hourly_df.to_csv(f, index=False)\n",
    "        f.write(\"\\nConnection Log\\n\")\n",
    "        chunk_df.to_csv(f, index=False)\n",
    "    \n",
    "output_file = os.path.join(folder_path, f\"{folder_name}_dropouts.csv\")\n",
    "create_connection_status_csv(accel_dt, chunk_start, chunk_end, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002380dc",
   "metadata": {},
   "source": [
    "## Class and Function Definitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1e49d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for Calculating Multiple Metrics\n",
    "\n",
    "def downsample_stages(x, original, target):\n",
    "    next_downsample = 1\n",
    "    num_down = 0\n",
    "    current = original\n",
    "    # While we want to downsample\n",
    "    while current / (next_downsample * 2) > target:\n",
    "       # Increase the downsample factor\n",
    "        next_downsample *= 2\n",
    "        num_down += 1\n",
    "        # If we don't want to use a greater downsample factor,\n",
    "        # Do the downsample\n",
    "        if next_downsample * 2 > 8:\n",
    "            x = signal.decimate(x, next_downsample)\n",
    "            current = current / next_downsample\n",
    "            next_downsample = 1\n",
    "\n",
    "    if (current/(next_downsample)-target) > (target-current/(next_downsample*2)):\n",
    "        x = signal.decimate(x, next_downsample*2)  # Final downsample\n",
    "        num_down += 1\n",
    "    else:\n",
    "        x = signal.decimate(x, next_downsample)  # Final downsample\n",
    "    return(x, 2**num_down)\n",
    "\n",
    "def accel_detrend(accel):\n",
    "    return accel - savgol_filter(accel, 21, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5e4dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate sampling rate for debugging #Do not use yet\n",
    "\n",
    "class SampleRate:\n",
    "    def __init__(self, chunk, time_SR, SR):\n",
    "        self.chunk = chunk                                # index of data range (used to index the all of the below arrays)\n",
    "        self.time_SR = time_SR                            # array of time value arrays corresponding to sample rate data\n",
    "        self.SR = SR                                      # array of sample rate data arrays\n",
    "\n",
    "def calculate_sampling_rate(chunk, accel_ms, accel_ms_old, accel_ms_diff):\n",
    "    window_n = params.vital_w*params.fs\n",
    "    time_seg_start_idx = np.arange(0,int(len(accel_ms)-window_n),int(np.floor(window_n*params.vital_ovlp)))\n",
    "\n",
    "    time = (time_seg_start_idx + window_n/2)+params.start_time\n",
    "\n",
    "    data = np.zeros(len(time_seg_start_idx))\n",
    "    for i in range(len(time_seg_start_idx)):\n",
    "        time_seg = accel_ms[time_seg_start_idx[i]:int(time_seg_start_idx[i]+window_n)]\n",
    "        idx = np.argwhere((accel_ms_old >= time_seg[0]) & (accel_ms_old < time_seg[-1])).flatten()\n",
    "        if idx.size > 0:\n",
    "            data = 1000/(np.mean(accel_ms_diff[idx]))\n",
    "        else:\n",
    "            data = 0 \n",
    "\n",
    "    return SampleRate(chunk, time, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a04ca15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes and Functions for Calculating Activity\n",
    "\n",
    "class Activity:\n",
    "    def __init__(self, chunk, time, data):\n",
    "        self.chunk = chunk                          # index of data range (used to index the all of the below arrays)\n",
    "        self.time = time                            # array of time value arrays corresponding to activity data\n",
    "        self.data = data                            # array of activity data arrays\n",
    "        \n",
    "#def calculate_activity(accel_x, accel_y, accel_z_clean):\n",
    "def calculate_activity(chunk, accel_x_chunk, accel_y_chunk, accel_z_clean):\n",
    "    act_window_N = window*params.fs_ds\n",
    "    act_overlap = 0.25\n",
    "    \n",
    "    down_x,_ = downsample_stages(accel_x_chunk[chunk],params.fs,params.downsample)\n",
    "    down_y,_ = downsample_stages(accel_y_chunk[chunk],params.fs,params.downsample)\n",
    "    down_z,_ = downsample_stages(accel_z_clean[chunk],params.fs,params.downsample)\n",
    "\n",
    "    act_index = np.arange(0,int(len(down_x)-act_window_N),int(np.floor(act_window_N*act_overlap)))\n",
    "\n",
    "    act_data = np.zeros((len(act_index)))\n",
    "\n",
    "    # act_time = (act_index + act_window_N/2)/params.fs_ds+params.start_time\n",
    "    \n",
    "    act_time = []\n",
    "    for i, idx in enumerate(act_index):\n",
    "        t = (idx + act_window_N / 2) / params.fs_ds + params.start_time\n",
    "        if np.isnan(t):\n",
    "            print(f\"NaN at index {i} (idx={idx})\")\n",
    "        act_time.append(t)\n",
    "\n",
    "    for i in range(len(act_index)):\n",
    "        x_window = down_x[act_index[i]:int(act_index[i]+act_window_N)]\n",
    "        y_window = down_y[act_index[i]:int(act_index[i]+act_window_N)]\n",
    "        z_window = down_z[act_index[i]:int(act_index[i]+act_window_N)]\n",
    "        act_data[i] = calculate_variance(x_window, y_window, z_window)\n",
    "\n",
    "    #return (act_time, activity)\n",
    "    return Activity(chunk, act_time, act_data)\n",
    "\n",
    "def calculate_variance(x, y, z):\n",
    "    sysVar = 4.38840806 * 10**-5\n",
    "    AI_prep = 0\n",
    "    AI_prep = AI_prep + ((np.std(x))**2 - sysVar)/sysVar\n",
    "    AI_prep = AI_prep + ((np.std(y))**2 - sysVar)/sysVar\n",
    "    AI_prep = AI_prep + ((np.std(z))**2 - sysVar)/sysVar\n",
    "    AI_prep = AI_prep * (1/3)\n",
    "    AI_prep = (max(AI_prep, 0))**.5\n",
    "    return(AI_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8c1e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes and Functions for Calculating Heart Rate\n",
    "\n",
    "class Cardiac:\n",
    "    def __init__(self, chunk, time_sig, sig, sig_sh, sig_sh_env, time_sig_ds, sig_sh_env_ds, sig_lt_ds, all_pks_sh, all_pks_lt, all_pks_matched, time_HR, HR, SQI):\n",
    "        self.chunk = chunk                          # index of data range (used to index the all of the below arrays)\n",
    "        self.time_sig = time_sig                    # array of time value arrays corresponding to attributes with \"sig\"\n",
    "        self.sig = sig                              # array of heart sound arrays\n",
    "        self.sig_sh = sig_sh                        # array of shannon envelope arrays (before filtering)\n",
    "        self.sig_sh_env = sig_sh_env                # array of shannon envelope arrays (after filtering)\n",
    "        self.time_sig_ds = time_sig_ds              # array of time value arrays corresponding to attributes with \"sig\" and \"ds\"\n",
    "        self.sig_sh_env_ds = sig_sh_env_ds          # array of shannon envelope arrays (after filtering) downsampled to 400 Hz\n",
    "        self.sig_lt_ds = sig_lt_ds                  # array of length transform arrays downsampled to 400 Hz\n",
    "        self.all_pks_sh = all_pks_sh                # array of arrays containing peak indices for sig_sh_env_ds\n",
    "        self.all_pks_lt = all_pks_lt                # array of arrays containing peak indices for sig_lt_ds\n",
    "        self.all_pks_matched = all_pks_matched      # array of arrays containing matched indices for all_pks_sh and all_pks_lt\n",
    "        self.time_HR = time_HR                      # array of time value arrays corresponding to HR and SQI\n",
    "        self.HR = HR                                # array of heart rate arrays\n",
    "        self.SQI = SQI                              # array of signal quality index arrays\n",
    "\n",
    "def autocorr(x):\n",
    "    result = signal.correlate(x,x)[len(x)-1:]\n",
    "    return result/result[0]    \n",
    "\n",
    "def AMPD_pks(x, min_scale=None, max_scale=None):\n",
    "    \"\"\"Find peaks in quasi-periodic noisy signals using ASS-AMPD algorithm.\n",
    "    AMPD_PKS Calculates the peaks of a periodic/quasi-periodic signal\n",
    "    Method adapted from Scholkmann et.al. (2012)\n",
    "    An Efficient Algorithm for Automatic Peak Detection in Noisy Periodic and\n",
    "    Quasi-Periodic Signals\"\"\"\n",
    "\n",
    "    x = signal.detrend(x)\n",
    "    N = len(x)\n",
    "\n",
    "    L = max_scale // 2\n",
    "    cut = min_scale // 2\n",
    "\n",
    "    # create LSM matix\n",
    "    LSM = np.ones((L, N), dtype=bool)\n",
    "    for k in np.arange(1, L + 1):\n",
    "        # compare to right neighbours\n",
    "        LSM[k - 1, 0:N - k] &= (x[0:N - k] > x[k:N])\n",
    "        LSM[k - 1, k:N] &= (x[k:N] > x[0:N - k])  # compare to left neighbours\n",
    "\n",
    "    G = LSM.sum(axis=1)\n",
    "    # normalize to adjust for new edge regions\n",
    "    G = G * np.arange(N // 2, N // 2 - L, -1)\n",
    "    l_scale = cut+np.argmax(G[cut:])\n",
    "\n",
    "    # find peaks that persist on all scales up to l\n",
    "    pks_logical = np.min(LSM[0:l_scale, :], axis=0)\n",
    "    pks = np.flatnonzero(pks_logical)\n",
    "\n",
    "    return pks\n",
    "\n",
    "def lengthtransform(x, w, fs):\n",
    "    # LENGTHTRANSFORM Computes the length transform of signal <sig>\n",
    "    # Length transform as described in Zong, Moody, Jiang (2003) A Robust\n",
    "    # Open-source Algorithm to Detect Onset and Duration of QRS Complexes.\n",
    "    # Length transform is simply the curve length with different windows <w>\n",
    "    # resulting in output LT as a function of window length and sample\n",
    "    C = 1/(fs**2)\n",
    "    w_N = int(np.ceil(w*fs))\n",
    "    normfactor = w_N/fs\n",
    "    dy_k = np.array(np.diff(x, prepend=0)).astype(float)\n",
    "    dL = np.sqrt(C + dy_k**2)\n",
    "\n",
    "    LT = dL.cumsum()\n",
    "    LT[w_N:] = LT[w_N:] - LT[:-w_N]\n",
    "    return(LT-normfactor)\n",
    "\n",
    "\n",
    "def shannon_energy_env(x):\n",
    "    x_env = -x**2 * np.log(x.astype(float)**2) #Original\n",
    "    #x_env = x #Just for testing, not necessary\n",
    "    return(x_env)\n",
    "\n",
    "# Heart Rate Calculations\n",
    "def calculate_b2b(envelope, length_transform, params):\n",
    "    pks_sh = AMPD_pks(envelope, min_scale=params.min_scale, max_scale=params.max_scale)\n",
    "    pks_lt = AMPD_pks(length_transform, min_scale=params.min_scale, max_scale=params.max_scale)\n",
    "    N_detected = min(len(pks_sh), len(pks_lt))\n",
    "    pks_sh = pks_sh[(envelope[pks_sh] > params.hs_thresh[0]) & (envelope[pks_sh] < params.hs_thresh[1])]\n",
    "\n",
    "    if N_detected:\n",
    "        residual = np.min(pks_sh[np.newaxis, :] - pks_lt[:, np.newaxis], axis=0)\n",
    "        matched = pks_sh[residual < .150*params.fs_ds]\n",
    "\n",
    "        # Find Beat to Beat Intervals\n",
    "        b2b = np.diff(matched)/params.fs_ds\n",
    "        # Check for accidental peaks in between real beats\n",
    "        for k in range(len(b2b)-1):\n",
    "            if b2b[k] < 60/params.hr[1] and b2b[k+1] < 60/params.hr[1]:\n",
    "                b2b[k] = b2b[k] + b2b[k+1]\n",
    "                b2b[k+1] = 0\n",
    "                matched[k] = 0\n",
    "        # Remove intervals outside expected HR range\n",
    "        b2b = b2b[(b2b < 60/params.hr[0]) & (b2b > 60/params.hr[1])]\n",
    "        matched = matched[matched > 0]\n",
    "        N_cleaned = len(b2b+1)\n",
    "\n",
    "        SQI = N_cleaned/N_detected\n",
    "    else:\n",
    "        matched, b2b, SQI = np.array([]), np.array([]), 0\n",
    "\n",
    "    return(pks_sh, pks_lt, matched, b2b, SQI)\n",
    "        \n",
    "def extract_cardiac(chunk, params, time, accel_z_clean):\n",
    "    time_sig = time\n",
    "    sos_hs = signal.butter(8,np.array(params.hs)/(params.fs/2),btype='bandpass',output='sos')\n",
    "    sos_shan = signal.butter(8,14/(params.fs/2),btype='low',output='sos')\n",
    "    sig = signal.sosfiltfilt(sos_hs,accel_z_clean[chunk]) # heart sound\n",
    "    \n",
    "    sig_lt = lengthtransform(sig,params.hs_len,params.fs) # heart sound length transform\n",
    "    sig_sh = shannon_energy_env(sig) # heart sound shannon envelope without filter\n",
    "    sig_sh_env = signal.sosfiltfilt(sos_shan,sig_sh) # heart sound shannon envelope with filter\n",
    "        \n",
    "    sig_lt_ds,ds_hs_lt = downsample_stages(sig_lt,params.fs,params.downsample)\n",
    "    sig_sh_env_ds,ds_hs_sh = downsample_stages(sig_sh_env,params.fs,params.downsample)\n",
    "    time_sig_ds = time[::2]\n",
    "\n",
    "    params.fs_ds =  params.fs/ds_hs_sh\n",
    "    \n",
    "    win_N = params.vital_w*params.fs_ds #number of samples in a single window\n",
    "    idx_v = np.arange(0,int(len(sig_sh_env_ds)-win_N),int(np.floor(win_N*params.vital_ovlp))) #index of hs_sh in which to start a new calculation\n",
    "    time_HR = (idx_v + win_N/2)/params.fs_ds+params.start_time #time in seconds of the centers of each window of hs_sh\n",
    "    \n",
    "    all_pks_matched = []\n",
    "    all_pks_sh = []\n",
    "    all_pks_lt = []\n",
    "\n",
    "    # Initialize starting search variables to capture correct signal features\n",
    "    params.min_scale = int(np.floor(60/params.hr[1]*params.fs_ds)) # Assuming initial heart rate is non-tachycardia\n",
    "    params.max_scale = int(np.ceil(60/params.hr[0]*params.fs_ds))\n",
    "    \n",
    "    HR = np.zeros(len(idx_v))\n",
    "    SQI = np.zeros(len(idx_v))\n",
    "    \n",
    "    # Loop through analysis windows\n",
    "    for i in range(len(idx_v)):\n",
    "        x_lt = sig_lt_ds[idx_v[i]:int(idx_v[i]+win_N)]\n",
    "        x_sh = sig_sh_env_ds[idx_v[i]:int(idx_v[i]+win_N)]\n",
    "        \n",
    "        pks_sh, pks_lt, pks_matched, b2b, SQI_temp = calculate_b2b(x_sh,x_lt,params)\n",
    "\n",
    "        # Saves detected heartbeats to global lists (debugging purpose)\n",
    "        if(len(pks_sh) > 0):\n",
    "            all_pks_sh = all_pks_sh + list((pks_sh + idx_v[i]))\n",
    "        if(len(pks_lt) > 0):\n",
    "            all_pks_lt = all_pks_lt + list((pks_lt + idx_v[i]))\n",
    "        if(len(pks_matched) > 0):\n",
    "            #all_matched = all_matched + list((matched + idx_v[i]))\n",
    "            all_pks_matched = all_pks_matched + list((pks_matched + idx_v[i]))\n",
    "\n",
    "        # This part uses autocorrelation to calculate the signal quality:\n",
    "        # If the data shows regular/periodical heartbeats, then it is high quality; if not, then low quality (motion artifacts or weak heartbeats from the sensor)\n",
    "        # The autocorrelation quantifies periodicity of signal.\n",
    "        ac = autocorr(x_sh)\n",
    "        win_hr = 0\n",
    "        SQI_temp = 0\n",
    "        ac_peaks, _ = signal.find_peaks(ac, distance = 60/params.hr[1]*params.fs_ds, height = 0.2, prominence = 0.1)\n",
    "        if ac_peaks.size > 0:\n",
    "            win_hr = 60/(ac_peaks[0]/params.fs_ds)\n",
    "            SQI_temp = ac[ac_peaks[0]]\n",
    "        else:\n",
    "            SQI_temp = 0\n",
    "\n",
    "        if (len(b2b)>1):\n",
    "            HR[i] = (60/np.mean(b2b))\n",
    "            SQI[i] = (SQI_temp)\n",
    "         \n",
    "    return Cardiac(chunk, time_sig, sig, sig_sh, sig_sh_env, time_sig_ds, sig_sh_env_ds, sig_lt_ds, all_pks_sh, all_pks_lt, all_pks_matched, time_HR, HR, SQI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab76995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes and Functions for Calculating Respiratory Rate\n",
    "        \n",
    "class Respiratory:\n",
    "    def __init__(self, chunk, time_sig, sig_prefilt, sig, pks, time_RR, RR):\n",
    "        self.chunk = chunk                          # index of data range (used to index the all of the below arrays)\n",
    "        self.time_sig = time_sig                    # array of time value arrays corresponding to attributes with \"sig\"\n",
    "        self.sig_prefilt = sig_prefilt              # array of respiratory signal arrays computed using envelope\n",
    "        self.sig = sig                              # array of respiratory signal arrays computed using bpf or envelope and bpf\n",
    "        self.pks = pks                              # array of arrays containing peak indices for sig\n",
    "        self.time_RR = time_RR                      # array of time value arrays corresponding to RR\n",
    "        self.RR = RR                                # array of respiratory rate arrays computed from pks\n",
    "\n",
    "def extract_respiratory(chunk, params, time, accel_x_chunk, accel_y_chunk, accel_z_chunk, all_pks_sh, all_pks_matched, sig_sh_env, accel_z_clean):\n",
    "\n",
    "    if not use_hs_to_calculate_RR:\n",
    "        time_sig = time[::16]\n",
    "        if params.resp_axis == 'x':\n",
    "            sig_prefilt, ds_f_resp = downsample_stages(accel_x_chunk,params.fs,params.ds_resp)\n",
    "        elif params.resp_axis == 'y':\n",
    "            sig_prefilt, ds_f_resp = downsample_stages(accel_y_chunk,params.fs,params.ds_resp)\n",
    "        elif params.resp_axis == 'z':\n",
    "            sig_prefilt, ds_f_resp = downsample_stages(accel_z_chunk,params.fs,params.ds_resp)\n",
    "        params.fs_resp =  params.fs/ds_f_resp\n",
    "        filt = signal.butter(8,[x/(params.fs_resp/2) for x in params.resp],btype='bandpass',output='sos')\n",
    "        sig = signal.sosfiltfilt(filt, sig_prefilt)\n",
    "\n",
    "        # Setup Output DataFrame\n",
    "        window_n = params.vital_w*params.fs_resp # 8s *50 Hz = 400 samples\n",
    "        sig_seg_start_idx = np.arange(0,int(len(sig_prefilt)-window_n),int(np.floor(window_n*params.vital_ovlp)))\n",
    "\n",
    "        time_RR = (sig_seg_start_idx + window_n/2)/params.fs_resp+params.start_time\n",
    "\n",
    "        pks = []\n",
    "\n",
    "        RR = np.zeros(len(sig_seg_start_idx))\n",
    "        \n",
    "        resp_peak_height = getattr(params, \"resp_peak_height\", None)\n",
    "        resp_peak_prominence = getattr(params, \"resp_peak_prominence\", None)\n",
    "        resp_peak_width = getattr(params, \"resp_peak_width\", None)\n",
    "        resp_peak_wlen = getattr(params, \"resp_peak_wlen\", None)\n",
    "\n",
    "        for i in range(len(sig_seg_start_idx)):\n",
    "            sig_seg = sig[sig_seg_start_idx[i]:int(sig_seg_start_idx[i] + window_n)]\n",
    "            pks_seg,_ = signal.find_peaks(sig_seg, \n",
    "                                          height=resp_peak_height,\n",
    "                                          distance=params.fs_resp/params.resp[1],\n",
    "                                          prominence=resp_peak_prominence, \n",
    "                                          width=resp_peak_width, \n",
    "                                          wlen=resp_peak_wlen)\n",
    "            pks = pks + list((pks_seg + sig_seg_start_idx[i]))\n",
    "            RR[i] = 60*params.fs_resp/np.mean(np.diff(pks_seg))#60/np.mean(resp) # Respiration Rate\n",
    "\n",
    "        return Respiratory(chunk, time_sig, sig_prefilt, sig, pks, time_RR, RR)\n",
    "                \n",
    "    elif use_hs_to_calculate_RR:\n",
    "        hs_pks_idx = []\n",
    "        if use_shannon_peaks_only_to_calculate_resp_env:\n",
    "            hs_pks_idx = np.array(all_pks_sh) #if only using hs_sh\n",
    "        else:\n",
    "            hs_pks_idx = np.array(all_pks_matched) #if using both sig_sh_env and sig_lt\n",
    "        hs_pks_idx, _ = np.unique(hs_pks_idx, return_index=True)\n",
    "        \n",
    "        hs_pks_idx = hs_pks_idx * params.fs / params.fs_ds\n",
    "        hs_pks_idx = hs_pks_idx.astype(int)\n",
    "\n",
    "        for i in range(len(hs_pks_idx)):\n",
    "            if(hs_pks_idx[i] > 7):\n",
    "                if use_shannon_peaks_only_to_calculate_resp_env:\n",
    "                    hs_pks_idx[i] = np.argmax (sig_sh_env[hs_pks_idx[i] - 8 : hs_pks_idx[i] + 8]) + (hs_pks_idx[i] - 8 ) #if only using sig_sh_env\n",
    "                else:\n",
    "                    hs_pks_idx[i] = np.argmax(accel_z_clean[hs_pks_idx[i] - 8 : hs_pks_idx[i] + 8]) + (hs_pks_idx[i] - 8 ) #if using both sig_sh_env and sig_lt\n",
    "        hs_pks_idx, unique_index = np.unique(hs_pks_idx, return_index=True)\n",
    "        \n",
    "        time_sig = time[::16]\n",
    "        idx_sig = time_sig * params.fs #50 Hz\n",
    "        \n",
    "        cs = None\n",
    "        if use_shannon_peaks_only_to_calculate_resp_env:\n",
    "            #cs = CubicSpline(hs_pks_idx, sig_sh_env[hs_pks_idx]) #if only using sig_sh_env\n",
    "            cs = CubicSpline(idx_sig[0] + hs_pks_idx, sig_sh_env[hs_pks_idx]) #if only using sig_sh_env\n",
    "        else:\n",
    "            #cs = CubicSpline(hs_pks_idx, accel_z_clean[hs_pks_idx]) #if using both sig_sh_env and sig_lt\n",
    "            cs = CubicSpline(idx_sig[0] + hs_pks_idx, accel_z_clean[hs_pks_idx]) #if using both sig_sh_env and sig_lt\n",
    "        \n",
    "        valid_mask = ((idx_sig[0] + hs_pks_idx[0]) <= idx_sig) & (idx_sig <= (idx_sig[0] + hs_pks_idx[-1]))  # Ensure idx_sig stays within bounds\n",
    "        time_sig = time_sig[valid_mask]  # Trim time_sig accordingly\n",
    "        idx_sig = idx_sig[valid_mask]    # Trim idx_sig accordingly\n",
    "        sig_prefilt = cs(idx_sig)\n",
    "        \n",
    "        filt = signal.butter(4,np.array(params.resp)/(params.ds_resp/2),btype='bandpass',output='sos')\n",
    "        sig = signal.sosfiltfilt(filt,sig_prefilt)\n",
    "\n",
    "        window_n = params.vital_w*50\n",
    "        sig_seg_start_idx = np.arange(0,int(len(sig_prefilt)-window_n),int(np.floor(window_n*params.vital_ovlp)))\n",
    "        \n",
    "        time_RR = (sig_seg_start_idx + window_n/2)/50+params.start_time\n",
    "\n",
    "        RR = np.zeros(len(sig_seg_start_idx))\n",
    "        pks = []\n",
    "        \n",
    "        resp_peak_height = getattr(params, \"resp_peak_height\", None)\n",
    "        resp_peak_prominence = getattr(params, \"resp_peak_prominence\", None)\n",
    "        resp_peak_width = getattr(params, \"resp_peak_width\", None)\n",
    "        resp_peak_wlen = getattr(params, \"resp_peak_wlen\", None)\n",
    "        \n",
    "        for i in range(len(sig_seg_start_idx)):\n",
    "            sig_seg = sig[sig_seg_start_idx[i]:int(sig_seg_start_idx[i]+window_n)]\n",
    "            pks_seg = []\n",
    "#             if(np.std(sig_seg) < 0.00001):\n",
    "#                 pks_seg,_ = signal.find_peaks(sig_seg,distance=params.fs/params.resp[1],prominence=.0000005)\n",
    "#             else:\n",
    "#                 pks_seg,_ = signal.find_peaks(sig_seg,distance=params.fs/params.resp[1],prominence=.00037, width=[0,250], wlen=300) \n",
    "            pks_seg, _ = signal.find_peaks(sig_seg, \n",
    "                                           height = resp_peak_height,\n",
    "                                           distance=50/params.resp[1],\n",
    "                                           prominence=resp_peak_prominence, \n",
    "                                           width=resp_peak_width, \n",
    "                                           wlen=resp_peak_wlen)\n",
    "        \n",
    "            pks = pks + list((pks_seg + sig_seg_start_idx[i]))\n",
    "            if len(pks_seg) > 1:\n",
    "                RR[i] = 60*50/np.mean(np.diff(pks_seg))\n",
    "    \n",
    "        return Respiratory(chunk, time_sig, sig_prefilt, sig, pks, time_RR, RR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d85eb0",
   "metadata": {},
   "source": [
    "## Analyze Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d885a6ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analyze Data\n",
    "\n",
    "#scale = 4096\n",
    "scale = 1\n",
    "    \n",
    "chunk_time = []\n",
    "accel_x_chunk = []\n",
    "accel_y_chunk = []\n",
    "accel_z_chunk = []\n",
    "chunk_temp_s = []\n",
    "chunk_temp = []\n",
    "\n",
    "time_activity = []\n",
    "activity = []\n",
    "\n",
    "x_resp = []\n",
    "all_pks_resp = []\n",
    "all_matched = []\n",
    "all_pks_sh = []\n",
    "all_pks_lt = []\n",
    "accel_z_clean = []\n",
    "accel_x_clean = []\n",
    "accel_y_clean = []\n",
    "hs = []\n",
    "hs_lt = []\n",
    "hs_sh_prefilt = []\n",
    "hs_sh = []\n",
    "chunk_time_ds = []\n",
    "chunk_time_rrds = []\n",
    "all_matched_idx_interp = []\n",
    "contour_interp = []\n",
    "contour_interp_filt = []\n",
    "all_pks_resp_heart = []\n",
    "all_std_xx = []\n",
    "\n",
    "time = []\n",
    "HR = []\n",
    "SQI = []\n",
    "RR = []\n",
    "RR_y = []\n",
    "RR_z = []\n",
    "RR_heart = []\n",
    "\n",
    "sampleRateX = {}\n",
    "activityX = {}\n",
    "cardiacX = {}\n",
    "respiratoryX = {}\n",
    "\n",
    "event_s_chunk = []\n",
    "event_chunk = []\n",
    "\n",
    "for chunk in range(num_chunks):\n",
    "    print(f\"Analyzing chunk {chunk+1} out of {num_chunks}\")\n",
    "    chunk_time.append(accel_ms[chunk_start[chunk]:chunk_end[chunk]] / 1000)\n",
    "\n",
    "    start_time = chunk_time[chunk][0]\n",
    "\n",
    "    accel_x_chunk.append(accel_x[chunk_start[chunk]:chunk_end[chunk]]/scale)\n",
    "    accel_y_chunk.append(accel_y[chunk_start[chunk]:chunk_end[chunk]]/scale)\n",
    "    accel_z_chunk.append(accel_z[chunk_start[chunk]:chunk_end[chunk]]/scale)\n",
    "\n",
    "    chunk_temp_s.append(temp_s[(temp_s >= chunk_time[chunk][0]) & (temp_s <= chunk_time[chunk][-1])])\n",
    "    chunk_temp.append(temp[(temp_s >= chunk_time[chunk][0]) & (temp_s <= chunk_time[chunk][-1])])\n",
    "\n",
    "    event_indices_chunk = np.where((event_ms >= accel_ms[chunk_start[chunk]]) & (event_ms < accel_ms[chunk_end[chunk]]))[0]\n",
    "\n",
    "    if event_indices_chunk.size > 0:\n",
    "        event_s_chunk.append(event_ms[event_indices_chunk] / 1000)\n",
    "        event_chunk.append(event[event_indices_chunk])\n",
    "    \n",
    "    params.start_time = start_time \n",
    "\n",
    "    accel_z_clean.append(accel_detrend(accel_z_chunk[chunk]))\n",
    "\n",
    "    #sampleRateX[chunk] = calculate_sampling_rate(chunk, chunk_time[chunk], accel_ms_old, accel_ms_diff)\n",
    "    cardiacX[chunk] = extract_cardiac(chunk, params, chunk_time[chunk], accel_z_clean)\n",
    "    respiratoryX[chunk] = extract_respiratory(chunk, params, chunk_time[chunk], accel_x_chunk[chunk], accel_y_chunk[chunk], accel_z_chunk[chunk], cardiacX[chunk].all_pks_sh, cardiacX[chunk].all_pks_matched, cardiacX[chunk].sig_sh_env, accel_z_clean[chunk])\n",
    "    activityX[chunk] = calculate_activity(chunk, accel_x_chunk, accel_y_chunk, accel_z_clean)\n",
    "    \n",
    "    #if use_hs_to_calculate_RR:\n",
    "        #To-Do: rename \"fs_ds\" to reflect that it is meant for heart rate, not respiratory rate. \n",
    "        #respiratoryX[chunk].time_sig = (respiratoryX[chunk].time_sig / params.fs_ds) + chunk_time[chunk][0];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256d6f0d",
   "metadata": {},
   "source": [
    "## Plot and Export Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7297906a",
   "metadata": {},
   "source": [
    "### Individual Chunk Time Plots/CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90913a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export data and plots for INDIVIDUAL chunks\n",
    "if plot_output:\n",
    "    for chunk in range(num_chunks):\n",
    "        print(f\"Exporting data and plots for chunk {chunk+1} out of {num_chunks}\")\n",
    "        idx_good = (cardiacX[chunk].SQI >= .0)\n",
    "        \n",
    "        plot_list = [plot_temperature, plot_accel_x, plot_accel_y, plot_accel_z, plot_activity, plot_accel_z_clean, plot_hs, plot_hs_sh_raw, \n",
    "                plot_hs_sh_filtered, plot_hs_lt, plot_HR, plot_SQI, plot_resp_sig_prefilt, plot_resp_sig, plot_RR]\n",
    "        try:\n",
    "            last_plot_index = max(index for index, value in enumerate(plot_list) if value)\n",
    "        except:\n",
    "            last_plot_index=0\n",
    "\n",
    "        if (plot_hs_sh_filtered & plot_resp_sig_prefilt & overlay_resp_sig_prefilt_w_hs & use_hs_to_calculate_RR):\n",
    "            num_plots = sum(plot_list) - 1\n",
    "        else:\n",
    "            num_plots = sum(plot_list)\n",
    "\n",
    "        fig, axes = plt.subplots(num_plots, 1, sharex=True)\n",
    "\n",
    "        if num_plots == 1:\n",
    "            axes = [axes]\n",
    "\n",
    "        Q1 = np.percentile(accel_z_clean[chunk], 5)\n",
    "        Q3 = np.percentile(accel_z_clean[chunk], 95)\n",
    "        IQR = Q3 - Q1\n",
    "        T1 = (accel_z_clean[chunk] >= (Q1 - 1.5 * (IQR - Q1)))\n",
    "        T2 = (accel_z_clean[chunk] <= Q3 + 1.5 * IQR)\n",
    "        accel_z_clean_no_outliers = accel_z_clean[chunk][T1 & T2]\n",
    "        accel_z_clean_min = min(accel_z_clean_no_outliers)\n",
    "        accel_z_clean_max = max(accel_z_clean_no_outliers)\n",
    "\n",
    "        hs_sh_no_outliers = cardiacX[chunk].sig_sh_env_ds[cardiacX[chunk].sig_sh_env_ds <= (Q3:= np.percentile(cardiacX[chunk].sig_sh_env_ds, 95)) + 1.5 * (IQR:= Q3 - (Q1 := np.percentile(cardiacX[chunk].sig_sh_env_ds, 5)))]\n",
    "        hs_sh_no_outliers_max = max(hs_sh_no_outliers)\n",
    "\n",
    "        hs_lt_no_outliers = cardiacX[chunk].sig_lt_ds[cardiacX[chunk].sig_lt_ds <= (Q3:= np.percentile(cardiacX[chunk].sig_lt_ds, 95)) + 1.5 * (IQR:= Q3 - (Q1 := np.percentile(cardiacX[chunk].sig_lt_ds, 5)))]\n",
    "        hs_lt_no_outliers_max = max(hs_lt_no_outliers)\n",
    "\n",
    "        plot_data = [\n",
    "            (chunk_temp_s[chunk], chunk_temp[chunk], 'm', [chunk_temp[chunk].min() - 1, chunk_temp[chunk].max() + 1], 'Temp\\n(C)', 'line'),\n",
    "            (chunk_time[chunk], accel_x_chunk[chunk], 'g', [-2, 2], 'Accel\\nX (g)', 'line'),\n",
    "            (chunk_time[chunk], accel_y_chunk[chunk], 'r', [-2, 2], 'Accel\\nY (g)', 'line'),\n",
    "            (chunk_time[chunk], accel_z_chunk[chunk], 'b', [-2, 2], 'Accel\\nZ (g)', 'line'),\n",
    "            (activityX[chunk].time, activityX[chunk].data, 'k', [0.0, 1.1*activityX[chunk].data.max()], 'Phys.\\nAct.\\n(a.u.)', 'line'),\n",
    "            (chunk_time[chunk], accel_z_clean[chunk], 'b', [accel_z_clean_min, accel_z_clean_max], 'Accel\\nZ\\nCln.', 'line'),\n",
    "            (cardiacX[chunk].time_sig, cardiacX[chunk].sig, 'b', [cardiacX[chunk].sig.min(), cardiacX[chunk].sig.max()], 'H.S.', 'line'),\n",
    "            (cardiacX[chunk].time_sig, cardiacX[chunk].sig_sh, 'b', [0, cardiacX[chunk].sig_sh.max()], 'H.S.\\nFilt.', 'line'),\n",
    "            (cardiacX[chunk].time_sig_ds, cardiacX[chunk].sig_sh_env_ds, 'b', [0, hs_sh_no_outliers_max], 'H.S.\\nShan.\\nEnv.', 'line'),\n",
    "            (cardiacX[chunk].time_sig_ds, cardiacX[chunk].sig_lt_ds, 'b', [0, hs_lt_no_outliers_max], 'H.S.\\nLen.\\nTrans.', 'line'),\n",
    "            (cardiacX[chunk].time_HR, cardiacX[chunk].HR, 'r',  [100, 800], 'HR\\n(bpm)', 'scatter'),\n",
    "            (cardiacX[chunk].time_HR, cardiacX[chunk].SQI, 'k', [0, 1], 'HR\\nSQI', 'scatter'),\n",
    "            (respiratoryX[chunk].time_sig, respiratoryX[chunk].sig_prefilt, 'k', [respiratoryX[chunk].sig_prefilt.min(),respiratoryX[chunk].sig_prefilt.max()], 'Resp.\\nSig.', 'line'),\n",
    "            (respiratoryX[chunk].time_sig, respiratoryX[chunk].sig, 'b', [respiratoryX[chunk].sig.min(), respiratoryX[chunk].sig.max()], 'Resp.\\nSig.\\nFilt.', 'line'),\n",
    "            (respiratoryX[chunk].time_RR, respiratoryX[chunk].RR, 'g', [0, respiratoryX[chunk].RR.max()], 'RR\\n(brpm)', 'scatter'),\n",
    "            #(time[chunk], all_std_xx[chunk], 'k', [0, 0.00003], 'std', 'scatter')\n",
    "            #(sampleRateX[chunk].time_SR, sampleRateX[chunk].SR, 'k', sampleRateX[chunk].SR.min(), sampleRateX[chunk].SR.max(), 'sampling rate', 'line')\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        plot_idx = 0\n",
    "        hs_sh_plot_idx = 0\n",
    "\n",
    "        legend_elements = []\n",
    "\n",
    "        for i, show_plot in enumerate(plot_list):\n",
    "            if show_plot:\n",
    "                plot_info = plot_data[i]\n",
    "                x_data, y_data, color, y_lim, y_label, plot_type = plot_info\n",
    "\n",
    "                if plot_type == 'line':\n",
    "                    if (y_label == 'H.S.\\nShan.\\nEnv.') & overlay_resp_sig_prefilt_w_hs & use_hs_to_calculate_RR:\n",
    "                        hs_sh_plot_idx = plot_idx\n",
    "                    if (y_label == 'Resp.\\nSig.') & plot_hs_sh_filtered & overlay_resp_sig_prefilt_w_hs & use_hs_to_calculate_RR:\n",
    "                        axes[hs_sh_plot_idx].plot(x_data, y_data, c=color, linewidth=0.5)\n",
    "                        continue\n",
    "                    else:\n",
    "                        axes[plot_idx].plot(x_data, y_data, c=color, linewidth=0.5)\n",
    "                    if (y_label == 'H.S.\\nShan.\\nEnv.') & plot_HR_peaks:\n",
    "                        for point in cardiacX[chunk].all_pks_sh:\n",
    "                            axes[plot_idx].scatter(cardiacX[chunk].time_sig_ds[point], cardiacX[chunk].sig_sh_env_ds[point], c='r', marker='o', s=3)\n",
    "                    if (y_label == 'Resp.\\nSig.\\nFilt.') & plot_RR_peaks:\n",
    "                        for point in respiratoryX[chunk].pks:\n",
    "                            axes[plot_idx].scatter(respiratoryX[chunk].time_sig[point], respiratoryX[chunk].sig[point], c='m', marker='o', s=3)\n",
    "                elif plot_type == 'scatter':\n",
    "                    axes[plot_idx].scatter(x_data[idx_good], y_data[idx_good], c=color, s=1, marker='o')\n",
    "\n",
    "                axes[plot_idx].set_ylim(y_lim)\n",
    "                axes[plot_idx].set_ylabel(y_label, fontsize=8)\n",
    "                axes[plot_idx].tick_params(axis='x', labelsize=8)\n",
    "                axes[plot_idx].tick_params(axis='y', labelsize=8)\n",
    "\n",
    "                if False:\n",
    "                    if chunk in local_chunk_event_times and local_chunk_event_times[chunk]:\n",
    "                        for event_time in local_chunk_event_times[chunk]:\n",
    "                            axes[plot_idx].plot([event_time, event_time], [y_lim[0], y_lim[1]], color='r', linewidth=0.5)\n",
    "\n",
    "                    if chunk in global_chunk_event_times and global_chunk_event_times[chunk]:\n",
    "                        for event_time in global_chunk_event_times[chunk]:\n",
    "                            axes[plot_idx].plot([event_time, event_time], [y_lim[0], y_lim[1]], color='k', linewidth=0.5)\n",
    "                if len(event_chunk) > 0:\n",
    "                    unique_events = np.unique(event_chunk[chunk])\n",
    "                    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_events)))\n",
    "                    event_color_dict = dict(zip(unique_events, colors))\n",
    "\n",
    "\n",
    "                    for event_name in unique_events:\n",
    "                        event_indices = event_chunk[chunk] == event_name\n",
    "                        event_times = event_s_chunk[chunk][event_indices]\n",
    "\n",
    "                        event_color = event_color_dict[event_name]\n",
    "                        for event_time in event_times:\n",
    "                            axes[plot_idx].plot([event_time, event_time], [y_lim[0], y_lim[1]], \n",
    "                                        color=event_color, linewidth=1, alpha=0.7)\n",
    "\n",
    "                        legend_elements.append(Line2D([0], [0], color=event_color, label=event_name))\n",
    "\n",
    "                if i == last_plot_index:\n",
    "                    ticks = np.linspace(chunk_time[chunk][0], chunk_time[chunk][-1], 7)\n",
    "\n",
    "                    #FOR DEBUG\n",
    "                    #axes[plot_idx].set_xlim([chunk_time[chunk][0], chunk_time[chunk][8000]])\n",
    "                    #ticks = np.linspace(chunk_time[chunk][0], chunk_time[chunk][8000], 7)\n",
    "\n",
    "                    tick_labels = [(accel_first_dt + timedelta(seconds=tick)).time().strftime(\"%H:%M:%S\") for tick in ticks]\n",
    "                    axes[plot_idx].set_xticks(ticks)\n",
    "                    axes[plot_idx].set_xticklabels(tick_labels)\n",
    "\n",
    "                #FOR DEBUG    \n",
    "                #axes[plot_idx].set_xlim([chunk_time[chunk][0], chunk_time[chunk][8000]])\n",
    "                #ticks = np.linspace(chunk_time[chunk][0], chunk_time[chunk][8000], 7)\n",
    "\n",
    "                plot_idx += 1\n",
    "\n",
    "        if legend_elements:        \n",
    "            axes[0].legend(handles=legend_elements, bbox_to_anchor=(0., 1.02, 1., .102), loc='lower right', ncols=2, borderaxespad=0.)     \n",
    "        fig.align_ylabels()\n",
    "        fig.subplots_adjust(left=0.15, right=0.95, top=0.95, bottom=0.08, hspace=0.4)\n",
    "\n",
    "        if scrolling:\n",
    "            ax_slider = plt.axes([0.12, 0.01, 0.76, 0.03])  # [left, bottom, width, height]\n",
    "            slider = Slider(ax_slider, 'Time (s)', chunk_time[chunk][0], chunk_time[chunk][-1] - scroll_window, valinit=scroll_window)\n",
    "\n",
    "            plot_idx = 0\n",
    "            for i, show_plot in enumerate(plot_list):\n",
    "                if show_plot:\n",
    "\n",
    "                    if i == last_plot_index:\n",
    "                        ticks = np.arange(np.floor(min(x_data)), np.ceil(max(x_data)) + 1, 1)\n",
    "                        tick_labels = [(accel_first_dt + timedelta(seconds=tick)).time().strftime(\"%H:%M:%S\") for tick in ticks]\n",
    "                        axes[plot_idx].set_xticks(ticks)\n",
    "                        axes[plot_idx].set_xticklabels(tick_labels)\n",
    "\n",
    "            # Function to update xlim based on slider value\n",
    "            def update(val):\n",
    "                new_xlim = slider.val, slider.val + scroll_window\n",
    "\n",
    "                plot_idx = 0\n",
    "                for i, show_plot in enumerate(plot_list):\n",
    "                    if show_plot and not(i == 12 and plot_hs_sh_filtered & plot_resp_sig_prefilt & overlay_resp_sig_prefilt_w_hs & use_hs_to_calculate_RR): #Do not increment subplot if resp_env_raw and hs_sh are overlayed\n",
    "                        axes[plot_idx].set_xlim(new_xlim)\n",
    "                        plot_idx += 1\n",
    "\n",
    "                fig.canvas.draw_idle()\n",
    "\n",
    "            update(0)\n",
    "\n",
    "            slider.on_changed(update)\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "        # png_output_file = os.path.join(folder_path, f\"{folder_name}_chunk_{chunk}_plot.png\")\n",
    "        # fig.savefig(png_output_file, dpi=300)\n",
    "        \n",
    "\n",
    "if export_to_csv:\n",
    "    for chunk in range(num_chunks):\n",
    "        chunk_dir = os.path.join(folder_path, f'chunk_{chunk}')\n",
    "        os.makedirs(chunk_dir, exist_ok=True)\n",
    "        \n",
    "        png_output_file = os.path.join(chunk_dir, f\"{folder_name}_chunk_{chunk}_time_plot.png\")\n",
    "        if plot_output:\n",
    "            fig.savefig(png_output_file, dpi=300)\n",
    "\n",
    "        accel_export = np.zeros((len(chunk_time[chunk]), 4))\n",
    "        accel_export[:,0] = chunk_time[chunk]\n",
    "        accel_export[:,1] = accel_x_chunk[chunk]\n",
    "        accel_export[:,2] = accel_y_chunk[chunk]\n",
    "        accel_export[:,3] = accel_z_chunk[chunk]\n",
    "        accel_export = accel_export[::100,:]\n",
    "        accel_export_headers = \"time (s),accel X (g),accel Y (g),accel Z (g)\"\n",
    "        accel_export_with_headers = np.vstack([accel_export_headers.split(','), accel_export])\n",
    "        accel_output_file = os.path.join(chunk_dir, f\"{folder_name}_chunk_{chunk}_accel.csv\")\n",
    "        np.savetxt(accel_output_file, accel_export_with_headers, delimiter=',', fmt='%s')\n",
    "\n",
    "        PA_export = np.zeros((len(activityX[chunk].time), 2))\n",
    "        PA_export[:,0] = activityX[chunk].time\n",
    "        PA_export[:,1] = activityX[chunk].data\n",
    "        PA_export_headers = \"time (s),PA (a.u.)\"\n",
    "        PA_export_with_headers = np.vstack([PA_export_headers.split(','), PA_export])\n",
    "        PA_output_file = os.path.join(chunk_dir, f\"{folder_name}_chunk_{chunk}_PA.csv\")\n",
    "        np.savetxt(PA_output_file, PA_export_with_headers, delimiter=',', fmt='%s')\n",
    "        \n",
    "        cardiac_df = pd.DataFrame({\n",
    "            \"Time (ms)\": np.round(cardiacX[chunk].time_HR, 3),\n",
    "            \"Heart Rate (bpm)\": np.round(cardiacX[chunk].HR, 3),\n",
    "            \"SQI (a.u.)\": np.round(cardiacX[chunk].SQI, 3),\n",
    "        })\n",
    "\n",
    "        # Save to CSV\n",
    "        cardiac_output_file = os.path.join(chunk_dir, f\"{folder_name}_chunk_{chunk}_cardiac.csv\")\n",
    "        cardiac_df.to_csv(cardiac_output_file, index=False)\n",
    "        \n",
    "        respiratory_df = pd.DataFrame({\n",
    "            \"Time (ms)\": np.round(respiratoryX[chunk].time_RR, 3),\n",
    "            \"Respiratory Rate (brpm)\": np.round(respiratoryX[chunk].RR, 3),\n",
    "        })\n",
    "\n",
    "        # Save to CSV\n",
    "        respiratory_output_file = os.path.join(chunk_dir, f\"{folder_name}_chunk_{chunk}_respiratory.csv\")\n",
    "        respiratory_df.to_csv(respiratory_output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24240c90",
   "metadata": {},
   "source": [
    "### Combined Chunk Time Plot/CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9a1c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export data and plots for MERGED chunks\n",
    "\n",
    "def remove_nans(data):\n",
    "    return np.array([x for x in data if x is not None and not math.isnan(x)])\n",
    "\n",
    "def merge_line(data):\n",
    "    return np.array([item for sublist in [item for sublist in data for item in (sublist, None)][:-1] for item in (sublist if sublist is not None else [None])])\n",
    "\n",
    "def merge_scatter(data):\n",
    "    return np.concatenate(data)\n",
    "\n",
    "def merge_object(data):\n",
    "    return np.array([item for sublist in [item for sublist in data for item in (sublist, None)][:-1] for item in (sublist if sublist is not None else [None])])\n",
    "\n",
    "def merge_peak(data, some_time):\n",
    "    combined_peaks = []\n",
    "    for offset, sublist in enumerate(data):\n",
    "        chunk_increment = sum(len(indices) for indices in some_time[:offset])\n",
    "        #need to shift peaks forward to accomodate for chunks (chunk_increment) and NaNs (offset)\n",
    "        adjusted_sublist = [index + offset + chunk_increment for index in sublist]\n",
    "        combined_peaks.extend(adjusted_sublist)\n",
    "    return np.array(combined_peaks)\n",
    "\n",
    "merged_chunk_time = merge_line(chunk_time)\n",
    "merged_chunk_time_ds = merge_line([item.time_sig_ds for item in cardiacX.values()])\n",
    "merged_chunk_time_rrds = merge_line([item.time_sig for item in respiratoryX.values()])\n",
    "\n",
    "merged_chunk_temp_s = merge_line(chunk_temp_s)\n",
    "\n",
    "merged_accel_x_chunk = merge_line(accel_x_chunk)\n",
    "merged_accel_y_chunk = merge_line(accel_y_chunk)\n",
    "merged_accel_z_chunk = merge_line(accel_z_chunk)\n",
    "merged_chunk_temp = merge_line(chunk_temp)\n",
    "merged_accel_z_clean = merge_line(accel_z_clean)\n",
    "\n",
    "merged_hs = merge_line([item.sig for item in cardiacX.values()])\n",
    "\n",
    "merged_hs_sh_prefilt = merge_line([item.sig_sh for item in cardiacX.values()]) \n",
    "merged_hs_sh = merge_line([item.sig_sh_env_ds for item in cardiacX.values()]) \n",
    "merged_hs_lt = merge_line([item.sig_lt_ds for item in cardiacX.values()]) \n",
    "    \n",
    "merged_time_activity = merge_line([item.time for item in activityX.values()])\n",
    "merged_activity = merge_line([item.data for item in activityX.values()])\n",
    "\n",
    "merged_time_HR = merge_scatter([item.time_HR for item in cardiacX.values()])\n",
    "merged_HR = merge_scatter([item.HR for item in cardiacX.values()])\n",
    "merged_SQI = merge_scatter([item.SQI for item in cardiacX.values()])\n",
    "\n",
    "merged_time_resp = merge_line([item.time_sig for item in respiratoryX.values()])\n",
    "merged_resp_prefilt = merge_line([item.sig_prefilt for item in respiratoryX.values()])\n",
    "merged_resp = merge_line([item.sig for item in respiratoryX.values()])\n",
    "merged_time_RR = merge_scatter([item.time_RR for item in respiratoryX.values()])\n",
    "merged_RR = merge_scatter([item.RR for item in respiratoryX.values()])\n",
    "\n",
    "merged_all_pks_resp = merge_peak([item.pks for item in respiratoryX.values()], [item.time_sig for item in respiratoryX.values()])\n",
    "merged_all_matched = merge_peak([item.all_pks_matched for item in cardiacX.values()], [item.time_sig_ds for item in cardiacX.values()])\n",
    "\n",
    "merged_all_pks_sh = merge_peak([item.all_pks_sh for item in cardiacX.values()], [item.time_sig_ds for item in cardiacX.values()])\n",
    "merged_all_pks_lt = merge_peak([item.all_pks_lt for item in cardiacX.values()], [item.time_sig_ds for item in cardiacX.values()])\n",
    "\n",
    "if len(event_chunk) > 0:\n",
    "    merged_event_chunk = merge_scatter(event_chunk)\n",
    "    merged_event_s_chunk = merge_scatter(event_s_chunk)\n",
    "else:\n",
    "    merged_event_chunk = []\n",
    "    merged_event_s_chunk = []\n",
    "    \n",
    "idx_good = (merged_SQI >= .0)\n",
    "\n",
    "plot_list = [plot_temperature, plot_accel_x, plot_accel_y, plot_accel_z, plot_activity, plot_accel_z_clean, plot_hs, plot_hs_sh_raw, \n",
    "            plot_hs_sh_filtered, plot_hs_lt, plot_HR, plot_SQI, plot_resp_sig_prefilt, plot_resp_sig, plot_RR]\n",
    "\n",
    "last_plot_index = max(index for index, value in enumerate(plot_list) if value) #Returns the highest index of the displayed plots\n",
    "\n",
    "if (plot_hs_sh_filtered & plot_resp_sig_prefilt & overlay_resp_sig_prefilt_w_hs):\n",
    "    num_plots = sum(plot_list) - 1\n",
    "else:\n",
    "    num_plots = sum(plot_list)\n",
    "    \n",
    "fig, axes = plt.subplots(num_plots, 1, sharex=True)\n",
    "\n",
    "if num_plots == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "accel_z_clean_no_outliers = remove_nans(merged_accel_z_clean)[(remove_nans(merged_accel_z_clean) >= (Q1 := np.percentile(remove_nans(merged_accel_z_clean), 5)) - 1.5 * (IQR := (Q3 := np.percentile(remove_nans(merged_accel_z_clean), 95)) - Q1)) & (remove_nans(merged_accel_z_clean) <= Q3 + 1.5 * IQR)]\n",
    "accel_z_clean_min = min(accel_z_clean_no_outliers)\n",
    "accel_z_clean_max = max(accel_z_clean_no_outliers)\n",
    "\n",
    "hs_sh_prefilt_no_outliers = remove_nans(merged_hs_sh_prefilt)[remove_nans(merged_hs_sh_prefilt) <= (Q3:= np.percentile(remove_nans(merged_hs_sh_prefilt), 99)) + 1.5 * (IQR:= Q3 - (Q1 := np.percentile(remove_nans(merged_hs_sh_prefilt), 1)))]\n",
    "hs_sh_prefilt_no_outliers_max = max(hs_sh_prefilt_no_outliers)\n",
    "\n",
    "hs_sh_no_outliers = remove_nans(merged_hs_sh)[remove_nans(merged_hs_sh) <= (Q3:= np.percentile(remove_nans(merged_hs_sh), 95)) + 1.5 * (IQR:= Q3 - (Q1 := np.percentile(remove_nans(merged_hs_sh), 5)))]\n",
    "hs_sh_no_outliers_max = max(hs_sh_no_outliers)\n",
    "\n",
    "hs_lt_no_outliers = remove_nans(merged_hs_lt)[remove_nans(merged_hs_lt) <= (Q3:= np.percentile(remove_nans(merged_hs_lt), 95)) + 1.5 * (IQR:= Q3 - (Q1 := np.percentile(remove_nans(merged_hs_lt), 5)))]\n",
    "hs_lt_no_outliers_max = max(hs_lt_no_outliers)\n",
    "\n",
    "if plot_output:\n",
    "    plot_data = [\n",
    "        (merged_chunk_temp_s, merged_chunk_temp, 'm', [min(remove_nans(merged_chunk_temp)) - 1, max(remove_nans(merged_chunk_temp)) + 1], 'Temp\\n(C)', 'line'),\n",
    "        (merged_chunk_time, merged_accel_x_chunk, 'g', [-2, 2], 'Accel\\nX (g)', 'line'),\n",
    "        (merged_chunk_time, merged_accel_y_chunk, 'r', [-2, 2], 'Accel\\nY (g)', 'line'),\n",
    "        (merged_chunk_time, merged_accel_z_chunk, 'b', [-2, 2], 'Accel\\nZ (g)', 'line'),\n",
    "        (merged_time_activity, merged_activity, 'k', [0.0, 1.1*remove_nans(merged_activity).max()], 'Phys.\\nAct.\\n(a.u.)', 'line'),\n",
    "        (merged_chunk_time, merged_accel_z_clean, 'b', [accel_z_clean_min, accel_z_clean_max], 'Accel\\nZ\\nCln.(g)', 'line'),\n",
    "        \n",
    "        (merged_chunk_time, merged_hs, 'b', [min(remove_nans(merged_hs)), max(remove_nans(merged_hs))], 'H.S.', 'line'), ###\n",
    "        (merged_chunk_time, merged_hs_sh_prefilt, 'b', [0, hs_sh_prefilt_no_outliers_max], 'H.S.\\nFilt.', 'line'),\n",
    "        (merged_chunk_time_ds, merged_hs_sh, 'b', [0, hs_sh_no_outliers_max], 'H.S.\\nShan.\\nEnv.', 'line'),\n",
    "        (merged_chunk_time_ds, merged_hs_lt, 'b', [0, hs_lt_no_outliers_max], 'H.S.\\nLen.\\nTrans.', 'line'),\n",
    "        (merged_time_HR, merged_HR, 'r',  [100, 800], 'HR\\n(bpm)', 'scatter'),\n",
    "        (merged_time_HR, merged_SQI, 'k', [0, 1], 'HR\\nSQI', 'scatter'),\n",
    "        \n",
    "        (merged_time_resp, merged_resp_prefilt, 'g', [min(remove_nans(merged_resp_prefilt)), max(remove_nans(merged_resp_prefilt))], 'Resp.\\nSig.', 'line'),\n",
    "        (merged_time_resp, merged_resp, 'g', [min(remove_nans(merged_resp)), max(remove_nans(merged_resp))], 'Resp.\\nSig.\\nFilt.', 'line'),\n",
    "        (merged_time_RR, merged_RR, 'g', [0, merged_RR.max()], 'RR\\n(brpm)', 'scatter'),\n",
    "    ]\n",
    "    \n",
    "    plot_idx = 0\n",
    "    hs_sh_plot_idx = 0\n",
    "\n",
    "    for i, show_plot in enumerate(plot_list):\n",
    "        if show_plot:\n",
    "            plot_info = plot_data[i]\n",
    "            x_data, y_data, color, y_lim, y_label, plot_type = plot_info\n",
    "            \n",
    "            if plot_type == 'line':\n",
    "                if (y_label == 'hs_sh') & overlay_resp_sig_prefilt_w_hs:\n",
    "                    hs_sh_plot_idx = plot_idx\n",
    "                if (y_label == 'x_resp_env') & plot_hs_sh_filtered & overlay_resp_sig_prefilt_w_hs:\n",
    "                    axes[hs_sh_plot_idx].plot(x_data, y_data, c=color, linewidth=0.5)\n",
    "                    if plot_RR_hs_env_peaks:\n",
    "                        for point in merged_all_pks_resp_heart:\n",
    "                            axes[hs_sh_plot_idx].scatter(merged_all_matched_idx_interp[point], merged_contour_interp_filt[point], c='k', marker='o', s=3)\n",
    "\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "                    axes[plot_idx].plot(x_data, y_data, c=color, linewidth=0.5)\n",
    "                if (y_label == 'hs_sh') & plot_HR_peaks:\n",
    "                    for point in merged_all_pks_sh:\n",
    "                        axes[plot_idx].scatter(merged_chunk_time_ds[point], merged_hs_sh[point], c='r', marker='o', s=3)\n",
    "                if (y_label == 'sig_resp') & plot_RR_peaks:\n",
    "                    for point in merged_all_pks_resp:\n",
    "                        axes[plot_idx].scatter(merged_time_resp[point], merged_resp[point], c='m', marker='o', s=3)\n",
    "            elif plot_type == 'scatter':\n",
    "                x_temp = np.array(x_data[idx_good])\n",
    "                y_temp = np.array(y_data[idx_good])\n",
    "                axes[plot_idx].scatter(x_temp, y_temp, c=color, s=1, marker='o')\n",
    "\n",
    "            axes[plot_idx].set_ylim(y_lim)\n",
    "            axes[plot_idx].set_ylabel(y_label, fontsize=8)\n",
    "            axes[plot_idx].tick_params(axis='x', labelsize=8)\n",
    "            axes[plot_idx].tick_params(axis='y', labelsize=8)\n",
    "\n",
    "            legend_elements = []\n",
    "            \n",
    "            if len(merged_event_chunk):\n",
    "                unique_events = np.unique(merged_event_chunk)\n",
    "                colors = plt.cm.tab10(np.linspace(0, 1, len(unique_events)))\n",
    "                event_color_dict = dict(zip(unique_events, colors))\n",
    "\n",
    "                \n",
    "                for event_name in unique_events:\n",
    "                    event_indices = merged_event_chunk == event_name\n",
    "                    event_times = merged_event_s_chunk[event_indices]\n",
    "\n",
    "                    event_color = event_color_dict[event_name]\n",
    "                    for event_time in event_times:\n",
    "                        axes[plot_idx].plot([event_time, event_time], [y_lim[0], y_lim[1]], \n",
    "                                    color=event_color, linewidth=0.5, alpha=0.7)\n",
    "\n",
    "                    legend_elements.append(Line2D([0], [0], color=event_color, label=event_name))\n",
    "                \n",
    "            if i == last_plot_index:\n",
    "                ticks = np.linspace(merged_chunk_time[0], merged_chunk_time[-1], 7)\n",
    "                tick_labels = [(accel_first_dt + timedelta(seconds=tick)).time().strftime(\"%H:%M:%S\") for tick in ticks]\n",
    "                axes[plot_idx].set_xticks(ticks)\n",
    "                axes[plot_idx].set_xticklabels(tick_labels)\n",
    "\n",
    "            plot_idx += 1\n",
    "\n",
    "    if legend_elements:        \n",
    "        axes[0].legend(handles=legend_elements, bbox_to_anchor=(0., 1.02, 1., .102), loc='lower right', ncols=2, borderaxespad=0.)     \n",
    "    \n",
    "    fig.align_ylabels()\n",
    "    fig.subplots_adjust(left=0.15, right=0.95, top=0.95, bottom=0.08, hspace=0.4)\n",
    "    \n",
    "    merged_dir = os.path.join(folder_path, 'chunk_all')\n",
    "    os.makedirs(merged_dir, exist_ok=True)\n",
    "    png_output_file = os.path.join(merged_dir, f\"{folder_name}_chunk_all_time_plot.png\")\n",
    "    fig.savefig(png_output_file, dpi=300)\n",
    "    \n",
    "if export_to_csv: \n",
    "    accel_export = np.zeros((len(merged_chunk_time), 4))\n",
    "    accel_export[:,0] = merged_chunk_time\n",
    "    accel_export[:,1] = merged_accel_x_chunk\n",
    "    accel_export[:,2] = merged_accel_y_chunk\n",
    "    accel_export[:,3] = merged_accel_z_chunk\n",
    "    accel_export = accel_export[::100,:]\n",
    "    accel_export_headers = \"time (s),accel X (g),accel Y (g),accel Z (g)\"\n",
    "    accel_export_with_headers = np.vstack([accel_export_headers.split(','), accel_export])\n",
    "    accel_output_file = os.path.join(merged_dir, f\"{folder_name}_chunk_all_accel.csv\")\n",
    "    np.savetxt(accel_output_file, accel_export_with_headers, delimiter=',', fmt='%s')\n",
    "\n",
    "    PA_export = np.zeros((len(merged_time_activity), 2))\n",
    "    PA_export[:,0] = merged_time_activity\n",
    "    PA_export[:,1] = merged_activity\n",
    "    PA_export_headers = \"time (s),PA (a.u.)\"\n",
    "    PA_export_with_headers = np.vstack([PA_export_headers.split(','), PA_export])\n",
    "    PA_output_file = os.path.join(merged_dir, f\"{folder_name}_chunk_all_PA.csv\")\n",
    "    np.savetxt(PA_output_file, PA_export_with_headers, delimiter=',', fmt='%s')\n",
    "\n",
    "    cardiac_df = pd.DataFrame({\n",
    "        \"Time (ms)\": np.round(merged_time_HR, 3),\n",
    "        \"Heart Rate (bpm)\": np.round(merged_HR, 3),\n",
    "        \"SQI (a.u.)\": np.round(merged_SQI, 3),\n",
    "    })\n",
    "\n",
    "    \n",
    "    # Save to CSV\n",
    "    cardiac_output_file = os.path.join(merged_dir, f\"{folder_name}_chunk_all_cardiac.csv\")\n",
    "    cardiac_df.to_csv(cardiac_output_file, index=False)\n",
    "    \n",
    "    respiratory_df = pd.DataFrame({\n",
    "        \"Time (ms)\": np.round(merged_time_RR, 3),\n",
    "        \"Respiratory Rate (brpm)\": np.round(merged_RR, 3),\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    respiratory_output_file = os.path.join(merged_dir, f\"{folder_name}_chunk_all_respiratory.csv\")\n",
    "    respiratory_df.to_csv(respiratory_output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9997ce95",
   "metadata": {},
   "source": [
    "### Combined Chunk Radar Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa2f97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime, timedelta\n",
    "import plotly.graph_objects as go\n",
    "from tkinter import Tk, filedialog\n",
    "import importlib.util\n",
    "import subprocess\n",
    "import sys\n",
    "import pkg_resources\n",
    "\n",
    "def ensure_kaleido_version(required_version=\"0.1.0.post1\"):\n",
    "    package = \"kaleido\"\n",
    "\n",
    "    try:\n",
    "        current_version = pkg_resources.get_distribution(package).version\n",
    "        if current_version != required_version:\n",
    "            print(f\"Kaleido version mismatch: {current_version} installed, {required_version} required.\")\n",
    "            print(\"Reinstalling correct version...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", f\"{package}=={required_version}\", \"--quiet\", \"--force-reinstall\"])\n",
    "            print(\"Kaleido installed successfully. Please restart the kernel.\")\n",
    "    except pkg_resources.DistributionNotFound:\n",
    "        print(\"Kaleido not installed. Installing required version...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", f\"{package}=={required_version}\", \"--quiet\"])\n",
    "        print(\"Kaleido installed successfully. Please restart the kernel.\")\n",
    "\n",
    "ensure_kaleido_version()\n",
    "\n",
    "RADAR_THEMES = {\n",
    "    'cardiac': {\n",
    "        'fill': 'rgba(0, 123, 255, 0.4)',       # Sky Blue\n",
    "        'line': 'rgb(0, 123, 255)'\n",
    "    },\n",
    "    'sqi': {\n",
    "        'fill': 'rgba(155, 89, 182, 0.4)',      # Violet Purple\n",
    "        'line': 'rgb(155, 89, 182)'\n",
    "    },\n",
    "    'PA': {\n",
    "        'fill': 'rgba(72, 201, 176, 0.4)',      # Mint Green\n",
    "        'line': 'rgb(72, 201, 176)'\n",
    "    },\n",
    "    'respiratory': {\n",
    "        'fill': 'rgba(255, 94, 77, 0.4)',       # Sunset Orange\n",
    "        'line': 'rgb(255, 94, 77)'\n",
    "    }\n",
    "}\n",
    "\n",
    "def polar_sector_path(start_hour, end_hour, radius=1.0, resolution=48):\n",
    "    total_points = resolution\n",
    "    points = []\n",
    "    for i in range(total_points + 1):\n",
    "        frac = i / total_points\n",
    "        hour = frac * 24\n",
    "        if start_hour <= hour <= end_hour:\n",
    "            angle = math.radians(90 - (hour / 24) * 360)\n",
    "            x = 0.5 + radius * math.cos(angle)\n",
    "            y = 0.5 + radius * math.sin(angle)\n",
    "            points.append((x, y))\n",
    "\n",
    "    if not points:\n",
    "        return \"\"\n",
    "\n",
    "    # Build SVG path\n",
    "    path = f\"M {points[0][0]},{points[0][1]} \"\n",
    "    for x, y in points[1:]:\n",
    "        path += f\"L {x},{y} \"\n",
    "    path += \"Z\"\n",
    "    return path\n",
    "\n",
    "\n",
    "def extract_start_time_from_filename(filename):\n",
    "    match = re.search(r'(\\d{8}_T\\d{6})', filename)\n",
    "    if match:\n",
    "        return datetime.strptime(match.group(1), '%Y%m%d_T%H%M%S')\n",
    "    return None\n",
    "\n",
    "def load_data(folder, keyword):\n",
    "    print(f\"Searching folder '{os.path.basename(folder)}' for keyword '{keyword}'\")\n",
    "    for file in os.listdir(folder):\n",
    "        if keyword in file and file.endswith('.csv'):\n",
    "            print(f\"Found file: {file}\")\n",
    "            filepath = os.path.join(folder, file).replace(\"\\\\\", \"/\")\n",
    "            \n",
    "            df = pd.read_csv(filepath)\n",
    "\n",
    "            if df.empty or df.shape[1] < 2:\n",
    "                continue\n",
    "\n",
    "            start_time = extract_start_time_from_filename(file)\n",
    "            if start_time is None:\n",
    "                continue\n",
    "\n",
    "            df.columns = ['time', keyword]\n",
    "            \n",
    "            #PA sometimes has nans, need to investigate\n",
    "            df = df[df['time'].notna()].copy()\n",
    "            \n",
    "            df['real_time'] = [start_time + timedelta(seconds=t) for t in df['time']]\n",
    "            return df[['real_time', keyword]]\n",
    "\n",
    "    return pd.DataFrame(columns=['real_time', keyword])\n",
    "\n",
    "def load_cardiac_data(folder):\n",
    "    print(f\"Searching folder '{os.path.basename(folder)}' for keyword 'cardiac'\")\n",
    "    for file in os.listdir(folder):\n",
    "        if 'cardiac' in file and file.endswith('.csv'):\n",
    "            print(f\"Found file: {file}\")\n",
    "            filepath = os.path.join(folder, file)\n",
    "\n",
    "            df = pd.read_csv(filepath)\n",
    "\n",
    "            if df.empty or df.shape[1] < 3:\n",
    "                continue\n",
    "\n",
    "            start_time = extract_start_time_from_filename(file)\n",
    "            if start_time is None:\n",
    "                continue\n",
    "\n",
    "            df.columns = ['time', 'cardiac', 'sqi']\n",
    "            df['real_time'] = [start_time + timedelta(seconds=t) for t in df['time']]  # ms to seconds\n",
    "\n",
    "            return df[['real_time', 'cardiac', 'sqi']]\n",
    "\n",
    "    return pd.DataFrame(columns=['real_time', 'cardiac', 'sqi'])\n",
    "\n",
    "def is_nighttime(label):\n",
    "    label = label.lstrip('⸺')\n",
    "    hour = int(label.split(':')[0])\n",
    "    return (hour >= 0 and hour < 6) or (hour >= 18 and hour <= 23)\n",
    "\n",
    "def make_radar_plot(df, value_col, title, save_file_name):\n",
    "    if df.empty:\n",
    "        print(f\"No data for {title}\")\n",
    "        return\n",
    "    \n",
    "    fill_color = RADAR_THEMES.get(value_col, {}).get('fill', 'rgba(0,0,0,0.3)')\n",
    "    line_color = RADAR_THEMES.get(value_col, {}).get('line', 'rgb(0,0,0)')\n",
    "\n",
    "    start_time = df['real_time'].min()\n",
    "    end_time = df['real_time'].max()\n",
    "    \n",
    "    total_duration = end_time - start_time\n",
    "    snippet = total_duration / 48\n",
    "    \n",
    "    df['time_block'] = ((df['real_time'] - start_time) // snippet) * snippet + start_time\n",
    "    df['label'] = df['time_block'].dt.strftime('%H:%M')\n",
    "\n",
    "    grouped = df.groupby('label')[value_col].mean()\n",
    "    \n",
    "    full_range = [start_time + i * snippet for i in range(48)]\n",
    "    full_labels = [t.strftime('%H:%M') for t in full_range]\n",
    "\n",
    "    grouped = grouped.reindex(full_labels).fillna(0)\n",
    "    grouped = grouped.reset_index()\n",
    "    grouped.columns = ['label', value_col]\n",
    "\n",
    "    categories_raw = grouped['label'].tolist()\n",
    "    \n",
    "\n",
    "    categories = [\n",
    "        label if i % 4 == 0 else f'⸺{label}'\n",
    "        for i, label in enumerate(categories_raw)\n",
    "    ]\n",
    "    \n",
    "    values = grouped[value_col].tolist()\n",
    "\n",
    "    categories += [categories[0]]\n",
    "    values += [values[0]]\n",
    "\n",
    "    fig = go.Figure(\n",
    "        data=go.Scatterpolar(\n",
    "            r=values,\n",
    "            theta=categories,\n",
    "            fill='toself',\n",
    "            name=title ,\n",
    "            fillcolor=fill_color,  \n",
    "            ) \n",
    "        )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        \n",
    "        polar=dict(\n",
    "            radialaxis=dict(\n",
    "                angle=-270,\n",
    "                visible=True,\n",
    "                range=[\n",
    "                    min([v for v in values if v > 0]) * 0.95 if any(v > 0 for v in values) else 0,\n",
    "                    max(values) * 1.05\n",
    "                ]\n",
    "            ),\n",
    "            angularaxis=dict(\n",
    "                tickmode='array',\n",
    "                tickvals=[i for i, label in enumerate(categories) if not label.startswith('⸺')],\n",
    "                ticktext = [\n",
    "                    f\"<span style='color:#8f8f8f; font-weight:bold'>{label}</span>\" if not is_nighttime(label)\n",
    "                    else f\"<span style='color:#000000; font-weight:bold'>{label}</span>\"\n",
    "                    for label in categories if not label.startswith('⸺')\n",
    "                ],\n",
    "                rotation=90,\n",
    "                direction='clockwise'\n",
    "            )\n",
    "        ),\n",
    "        showlegend=False,\n",
    "        title=title\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "    if chunk_all_folder and save_file_name:\n",
    "        save_path=os.path.join(chunk_all_folder, f\"{save_file_name}.png\").replace(\"\\\\\", \"/\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        print(f\"Saving radar chart to {save_path}\")\n",
    "        try:\n",
    "            fig.write_image(save_path, width=800, height=600, format='png')\n",
    "            print(f\"Saved radar chart to {save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to save radar chart: {e}\")\n",
    "    \n",
    "def select_folder():\n",
    "    root = Tk()\n",
    "    root.withdraw()\n",
    "    root.attributes(\"-topmost\", True)\n",
    "    trial_folder = filedialog.askdirectory(title=\"Select folder containing chunk folder(s)\")\n",
    "    trial_folder_name = os.path.basename(trial_folder)\n",
    "    chunk_all_folder = os.path.join(trial_folder, f'chunk_all').replace(\"\\\\\", \"/\")\n",
    "    return trial_folder_name, chunk_all_folder\n",
    "\n",
    "try:\n",
    "    print(\"trying to get path\")\n",
    "    folder_path\n",
    "    trial_folder_name = folder_name\n",
    "    chunk_all_folder = os.path.join(folder_path, \"chunk_all\").replace(\"\\\\\", \"/\")\n",
    "    print(f\"chunk_all_folder: {chunk_all_folder}\")\n",
    "except NameError:\n",
    "    print(\"could not get path, requesting manual entry\")\n",
    "    trial_folder_name, chunk_all_folder = select_folder()\n",
    "\n",
    "print(f\"trial folder name: {trial_folder_name}\")\n",
    "\n",
    "if chunk_all_folder:\n",
    "    df_cardiac = load_cardiac_data(chunk_all_folder)\n",
    "    df_pa = load_data(chunk_all_folder, 'PA')\n",
    "    df_resp = load_data(chunk_all_folder, 'respiratory')\n",
    "\n",
    "    make_radar_plot(df_cardiac[['real_time', 'cardiac']], 'cardiac', 'Average Heart Rate (BPM)', (trial_folder_name + f'_chunk_all_radar_plot_cardiac'))\n",
    "    make_radar_plot(df_cardiac[['real_time', 'sqi']], 'sqi', 'Heart Rate Signal Quality Index (SQI)', (trial_folder_name + f'_chunk_all_radar_plot_SQI'))\n",
    "    make_radar_plot(df_pa, 'PA', 'Average Physical Activity (a.u.)', (trial_folder_name + f'_chunk_all_radar_plot_PA'))\n",
    "    make_radar_plot(df_resp, 'respiratory', 'Average Respiratory Rate (BrPM)', (trial_folder_name + f'_chunk_all_radar_plot_respiratory'))\n",
    "else:\n",
    "    print(\"No folder selected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650e21a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
